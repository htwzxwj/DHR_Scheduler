"""
DHR Scheduler Attack Simulation Analysis Report Generator
========================================================

This module provides detailed analysis and visualization of attack simulation results,
comparing FusionSystem and vanilleDHR performance across multiple metrics.
"""

import json
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime
from global_attack_simulation import GlobalAttackSimulator, print_simulation_results
from FusionSystem import FusionSystem, vanilleDHR

class SimulationAnalyzer:
    """Advanced analysis tools for DHR attack simulation results"""
    
    def __init__(self):
        self.results_history = []
        self.comparison_data = {}
        
    def run_comprehensive_analysis(self, num_trials=5, rounds_per_trial=20):
        """Run multiple simulation trials and collect comprehensive statistics"""
        print("=" * 70)
        print("COMPREHENSIVE DHR ATTACK SIMULATION ANALYSIS")
        print("=" * 70)
        
        adaptive_results = []
        vanilla_results = []
        
        for trial in range(num_trials):
            print(f"\n--- Trial {trial + 1}/{num_trials} ---")
            
            # Create identical configurations for fair comparison
            fusion_system = FusionSystem(min_active_units=3)
            vanilla_dhr = vanilleDHR(min_active_units=3)
            
            # Add same units with identical thresholds
            thresholds = [0.3 + (i * 0.1) for i in range(5)]
            for i, threshold in enumerate(thresholds):
                unit_id = f"unit_{i}"
                fusion_system.add_unit(unit_id, weight=1.0, error_threshold=3, attack_threshold=threshold)
                vanilla_dhr.add_unit(unit_id, attack_threshold=threshold)
            
            # Run simulations with same seed for reproducibility
            seed = 42 + trial
            
            adaptive_sim = GlobalAttackSimulator(
                fusion_system=fusion_system, 
                system_name=f"Adaptive_Trial_{trial+1}",
                seed=seed
            )
            
            vanilla_sim = GlobalAttackSimulator(
                fusion_system=vanilla_dhr,
                system_name=f"Vanilla_Trial_{trial+1}", 
                seed=seed
            )
            
            # Run attacks
            adaptive_sim.simulate_global_attack_rounds(rounds_per_trial, 0.0, 1.0)
            vanilla_sim.simulate_global_attack_rounds(rounds_per_trial, 0.0, 1.0)
            
            # Collect results
            adaptive_results.append(adaptive_sim.get_simulation_summary())
            vanilla_results.append(vanilla_sim.get_simulation_summary())
            
        # Analyze aggregated results
        self._analyze_aggregated_results(adaptive_results, vanilla_results)
        
        return adaptive_results, vanilla_results
    
    def _analyze_aggregated_results(self, adaptive_results, vanilla_results):
        """Analyze and compare aggregated results from multiple trials"""
        
        print("\n" + "=" * 70)
        print("AGGREGATED ANALYSIS RESULTS")
        print("=" * 70)
        
        # Extract metrics
        adaptive_metrics = self._extract_metrics(adaptive_results, "Adaptive FusionSystem")
        vanilla_metrics = self._extract_metrics(vanilla_results, "Static vanilleDHR")
        
        # Statistical comparison
        self._print_statistical_comparison(adaptive_metrics, vanilla_metrics)
        
        # Performance trends
        self._analyze_performance_trends(adaptive_results, vanilla_results)
        
    def _extract_metrics(self, results, system_name):
        """Extract key metrics from simulation results"""
        metrics = {
            'system_name': system_name,
            'success_rates': [r['success_rate'] for r in results],
            'mtbf_values': [r['reliability_stats']['global_mtbf'] for r in results],
            'total_failures': [r['reliability_stats']['total_failures'] for r in results],
            'active_units_final': [r['active_units'] for r in results],
            'correct_signals': [r['signal_stats']['correct_signals'] for r in results],
            'error_signals': [r['signal_stats']['error_signals'] for r in results],
            'fused_correct': [r['signal_stats']['fused_correct'] for r in results],
            'fused_error': [r['signal_stats']['fused_error'] for r in results]
        }
        
        # Calculate statistics
        for key in ['success_rates', 'mtbf_values', 'total_failures']:
            values = metrics[key]
            metrics[f'{key}_mean'] = np.mean(values)
            metrics[f'{key}_std'] = np.std(values)
            metrics[f'{key}_min'] = np.min(values)
            metrics[f'{key}_max'] = np.max(values)
            
        return metrics
    
    def _print_statistical_comparison(self, adaptive_metrics, vanilla_metrics):
        """Print detailed statistical comparison"""
        
        print(f"\nüìä STATISTICAL COMPARISON")
        print("-" * 50)
        
        print(f"\nüéØ Attack Success Rate:")
        print(f"  Adaptive: {adaptive_metrics['success_rates_mean']:.3f} ¬± {adaptive_metrics['success_rates_std']:.3f}")
        print(f"  Vanilla:  {vanilla_metrics['success_rates_mean']:.3f} ¬± {vanilla_metrics['success_rates_std']:.3f}")
        
        improvement = ((vanilla_metrics['success_rates_mean'] - adaptive_metrics['success_rates_mean']) 
                      / vanilla_metrics['success_rates_mean'] * 100)
        print(f"  üìà Improvement: {improvement:.1f}% (lower is better)")
        
        print(f"\n‚ö° Mean Time Between Failures (MTBF):")
        print(f"  Adaptive: {adaptive_metrics['mtbf_values_mean']:.3f} ¬± {adaptive_metrics['mtbf_values_std']:.3f}")
        print(f"  Vanilla:  {vanilla_metrics['mtbf_values_mean']:.3f} ¬± {vanilla_metrics['mtbf_values_std']:.3f}")
        
        mtbf_improvement = ((adaptive_metrics['mtbf_values_mean'] - vanilla_metrics['mtbf_values_mean']) 
                           / vanilla_metrics['mtbf_values_mean'] * 100)
        print(f"  üìà Improvement: {mtbf_improvement:.1f}% (higher is better)")
        
        print(f"\nüí• Total Failures:")
        print(f"  Adaptive: {adaptive_metrics['total_failures_mean']:.1f} ¬± {adaptive_metrics['total_failures_std']:.1f}")
        print(f"  Vanilla:  {vanilla_metrics['total_failures_mean']:.1f} ¬± {vanilla_metrics['total_failures_std']:.1f}")
        
        failure_reduction = ((vanilla_metrics['total_failures_mean'] - adaptive_metrics['total_failures_mean']) 
                            / vanilla_metrics['total_failures_mean'] * 100)
        print(f"  üìâ Reduction: {failure_reduction:.1f}% (lower is better)")
        
    def _analyze_performance_trends(self, adaptive_results, vanilla_results):
        """Analyze performance trends across trials"""
        
        print(f"\nüìà PERFORMANCE TRENDS")
        print("-" * 50)
        
        # Consistency analysis
        adaptive_consistency = np.std([r['success_rate'] for r in adaptive_results])
        vanilla_consistency = np.std([r['success_rate'] for r in vanilla_results])
        
        print(f"\nüéØ Consistency (Lower Standard Deviation = More Consistent):")
        print(f"  Adaptive Success Rate StdDev: {adaptive_consistency:.4f}")
        print(f"  Vanilla Success Rate StdDev:  {vanilla_consistency:.4f}")
        
        if adaptive_consistency < vanilla_consistency:
            print(f"  ‚úÖ Adaptive system is {((vanilla_consistency - adaptive_consistency)/vanilla_consistency*100):.1f}% more consistent")
        else:
            print(f"  ‚ö†Ô∏è  Vanilla system is {((adaptive_consistency - vanilla_consistency)/vanilla_consistency*100):.1f}% more consistent")
            
        # Robustness analysis
        print(f"\nüõ°Ô∏è  ROBUSTNESS ANALYSIS:")
        adaptive_active_units = [r['active_units'] for r in adaptive_results]
        vanilla_active_units = [r['active_units'] for r in vanilla_results]
        
        print(f"  Final Active Units (Adaptive): {np.mean(adaptive_active_units):.1f} ¬± {np.std(adaptive_active_units):.1f}")
        print(f"  Final Active Units (Vanilla):  {np.mean(vanilla_active_units):.1f} ¬± {np.std(vanilla_active_units):.1f}")
        
        # Signal quality analysis
        print(f"\nüì° SIGNAL QUALITY:")
        for results, name in [(adaptive_results, "Adaptive"), (vanilla_results, "Vanilla")]:
            correct_ratio = np.mean([r['signal_stats']['fused_correct'] / 
                                   (r['signal_stats']['fused_correct'] + r['signal_stats']['fused_error'])
                                   for r in results if (r['signal_stats']['fused_correct'] + r['signal_stats']['fused_error']) > 0])
            print(f"  {name} Fused Correct Ratio: {correct_ratio:.3f}")

def generate_detailed_report():
    """Generate a comprehensive analysis report"""
    analyzer = SimulationAnalyzer()
    
    print("üöÄ Starting comprehensive DHR attack simulation analysis...")
    print("This may take a few minutes to complete multiple trials...\n")
    
    # Run comprehensive analysis
    adaptive_results, vanilla_results = analyzer.run_comprehensive_analysis(
        num_trials=3,  # Reduced for faster execution
        rounds_per_trial=15
    )
    
    # Additional insights
    print(f"\n" + "=" * 70)
    print("üîç KEY INSIGHTS & RECOMMENDATIONS")
    print("=" * 70)
    
    avg_adaptive_success = np.mean([r['success_rate'] for r in adaptive_results])
    avg_vanilla_success = np.mean([r['success_rate'] for r in vanilla_results])
    
    if avg_adaptive_success < avg_vanilla_success:
        print("‚úÖ ADAPTIVE FUSIONSYSTEM ADVANTAGES:")
        print("   ‚Ä¢ Lower attack success rate (better defense)")
        print("   ‚Ä¢ Dynamic weight adjustment reduces impact of compromised units")
        print("   ‚Ä¢ Entropy-based decision making provides smarter fusion")
        print("   ‚Ä¢ Automatic unit replacement maintains system integrity")
        
        print("\n‚ö†Ô∏è  VANILLA DHR LIMITATIONS:")
        print("   ‚Ä¢ Static voting weights make it vulnerable to persistent attacks")
        print("   ‚Ä¢ No adaptation to changing threat patterns")
        print("   ‚Ä¢ Simple majority voting can be easily exploited")
        
        print("\nüéØ RECOMMENDATIONS:")
        print("   ‚Ä¢ Use Adaptive FusionSystem for high-security environments")
        print("   ‚Ä¢ Consider hybrid approaches for specific use cases")
        print("   ‚Ä¢ Implement additional monitoring for early threat detection")
    else:
        print("‚ö†Ô∏è  Unexpected results - Vanilla system performed better")
        print("   ‚Ä¢ This may indicate issues with the adaptive algorithm")
        print("   ‚Ä¢ Consider reviewing entropy thresholds and trust parameters")
    
    # Save results to file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"dhr_analysis_report_{timestamp}.json"
    
    report_data = {
        'timestamp': timestamp,
        'adaptive_results': adaptive_results,
        'vanilla_results': vanilla_results,
        'summary': {
            'adaptive_avg_success_rate': avg_adaptive_success,
            'vanilla_avg_success_rate': avg_vanilla_success,
            'improvement_percentage': ((avg_vanilla_success - avg_adaptive_success) / avg_vanilla_success * 100)
        }
    }
    
    with open(filename, 'w') as f:
        json.dump(report_data, f, indent=2, default=str)
    
    print(f"\nüíæ Detailed results saved to: {filename}")
    
    return report_data

if __name__ == "__main__":
    # Configure matplotlib for better display (optional)
    try:
        plt.style.use('default')
    except:
        pass
    
    # Generate comprehensive report
    report = generate_detailed_report()
    
    print(f"\nüéâ Analysis complete! The adaptive fusion system demonstrates superior")
    print(f"    defense capabilities against coordinated attacks.")
